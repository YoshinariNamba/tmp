---
title: "教師あり機械学習 draft"
author: "Yoshinari Namba"
date: "2021/01/04"
output: 
  github_document: 
    pandoc_args: --webtex
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## 0. 準備
### パッケージ
```{r, warning=FALSE, message=FALSE}
## libary
library(tidyverse)
library(tidymodels)
library(glmnet)
library(stargazer)
```


### データ
ボストンの住宅価格データを使う。
```{r}
## data source
data("Boston", package = "MASS")
df <- Boston
rm(Boston)
```

目的変数と予測変数を別のデータセットとして格納。予測変数の交差項・2乗項を作成。
```{r}
## 予測変数(説明変数)を抽出
X_simple <- df %>% 
  select(-medv)

## 予測変数の交差項と2乗項を新たな予測変数として作成
X <- recipe(~ ., data = X_simple) %>%
  step_interact(~all_predictors():all_predictors()) %>% # Make interactions
  step_poly(crim, zn, indus) %>% # Make 2nd order terms
  step_nzv(all_predictors()) %>%  # Remove nearly zero variance variables
  step_lincomb(all_predictors()) %>% # Remove linear combinations
  prep() %>% 
  bake(X_simple) %>% 
  as.data.frame()

## データフレームに変換
X_simple <- as.data.frame(X_simple)

## 目的変数
y <- df$medv

```
- 観察単位: 町
- 変数 (`?MASS::Boston`参照)   
-- medv: median value of owner-occupied homes in \$1000s (目的変数)   
-- crim: per capita crime rate by town  
-- zn: proportion of residential land zone for lots over 25,000 sq.ft.  
-- indus: proportion of non-retail business acre per town  
-- chas: Charles River dummy variable  
-- nox: nitrogen oxides concentration  
-- rm: average room per dwelling  
-- age: proportion of owner-occupied units built prior to 1940  
-- dis: weighed mean of distances to five Boston employment centers  
-- rad: index of accessibility to radial highways  
-- tax: full-value property-tax rate per \$10,000  
-- ptratio: pupil-teacher ratio by town  
-- black: $1000(Bk - 0.63)^2$ where Bk is the proportion of blacks by town  
-- lstat: lower status of the population (percent)  


```{r}
df %>% stargazer(type = "text")
```

  

## 1. OLSの実行

### 1-1. 学習
シンプルなモデルのAdjusted R2
```{r}
mdl_lm_simple <- lm(data = X_simple, formula = y ~ .)
summary(mdl_lm_simple)$adj.r.squared
```


複雑なモデルのAdjusted R2 (交差項と2乗項を予測変数に含めたモデル)
```{r}
mdl_lm_complex <- lm(data = X, formula = y ~ .)
summary(mdl_lm_complex)$adj.r.squared
```

### 1-2. 予測誤差 (Mean Squared Error; MSE)
交差項や2乗項を予測変数として含めた複雑なモデルの方が予測誤差(MSE)が小さい．
```{r}
# modelling
mse_lm_simple <- sum( ( y - predict(mdl_lm_simple, X_simple) )^2 ) / length(y)
mse_lm_complex <- sum( ( y - predict(mdl_lm_complex, X) )^2 ) / length(y)

# summary
mse_lm <- data.frame(simple = mse_lm_simple, complex = mse_lm_complex)
rownames(mse_lm) <- "MSE"

mse_lm
```

  

## 2. Penalized Regressionの方法論

### 2-1. Bias-Variance Tradeoff

$y,X$の間に$y=f(X)+\epsilon$という関係があると仮定する($\epsilon$は平均$0$, 分散$\sigma^2$の誤差)．データ$D$から$f(X)$を近似する$\hat{f}(X; D)$というモデルを構築して新たな$X$から$y$を予測する．このとき予測誤差の期待値はBias($Bias_D(.)^2$)とVariance($Var_{D}(.)$), Noise($\sigma^2$)に分解できる．

```{=latex}
\begin{align}
  E_{D, \epsilon }\left[(y - \hat{f}(X ; D))^2 \right] &= \left[ Bias_{D} ( \hat{f} (X ; D) ) \right]^2 +  Var_{D} \left[ \hat{f} (X ; D) \right] + \sigma^2
\end{align}
```

ここで，
```{=latex}
\begin{align}
  Bias_{D} ( \hat{f} (X ; D) ) 
  &= E_{D}(\hat{f}(X;D)) - f(X) \\
  Var_{D} \left[ \hat{f} (X ; D) \right]
  &= E_{D} \left[ ( E_{D}[\hat{f}(X; D)] - \hat{f}(X; D) )^2 \right].
\end{align}
```

学習に使用したデータとは別のinputから予測する場合は，モデルを複雑化するとBiasは縮小する一方でVarianceが拡大するトレードオフの関係がある．


### 2-2. Ridge回帰・Lasso回帰

OLSは「誤差の2乗和」を最小化するのに対し，Ridge回帰 (Lasso回帰)では「誤差にパラメータの2乗(絶対値)を加えた値」を最小化する．パラメータの2乗(絶対値)はモデルの複雑さに対する罰則(penalty)である．
```{=latex}
\begin{align}
  OLS &: \min_{\beta} \  (y - X \beta)^{T}(y - X \beta) \\
  Ridge &: \min_{\beta} \  (y - X \beta)^{T}(y - X \beta) + \lambda \beta^{T}\beta \\
  Lasso &: \min_{\beta} \  (y - X \beta)^{T}(y - X \beta) + \lambda || \beta ||_1
\end{align}
```

ここでハイパーパラメータ$\lambda$は交差検証(後述)によって予測誤差を最小化する値にチューニングされる．


## 3. 交差検証 (Cross-Validation)

### 3-1. 交差検証による予測誤差の評価
未知のtargetを予測する際，biasのみならずvarianceも縮小させたい．そこで，手元のデータを「学習データ(training data)」と予測精度を評価する「テストデータ(test data)」に分割してモデルを吟味する「交差検証」を行う．学習データとテストデータはだいたい7:3ないし8:2に分割することが多い．
```{r}
set.seed(2022)
N_train <- round(length(y)*0.7) #  # of training observations 
id_train <- sample(x = length(y), size = N_train, replace = FALSE) # IDs of training observations

# training data
y_train <- y[id_train]
X_train <- X[id_train, ]

# test data
y_test <- y[-id_train]
X_test <- X[-id_train, ]

```

OLS, Ridge回帰, Lasso回帰に基づいてtraining dataからモデルを学習し，test dataからモデルの予測誤差(MSE)を算出してモデルを評価する．
```{r}
## modelling
mdl_lm <- lm(data = X_train, formula = y_train ~.)
mdl_ridge <- cv.glmnet(x = as.matrix(X_train), y = y_train, alpha = 1)
mdl_lasso <- cv.glmnet(x = as.matrix(X_train), y = y_train, alpha = 0)

## test
MSE_lm <- sum( (y_test - predict(mdl_lm, X_test))^2 ) / length(y)
MSE_ridge <- sum( (y_test - as.vector(predict(mdl_ridge, as.matrix(X_test))))^2 ) / length(y)
MSE_lasso <- sum( (y_test - as.vector(predict(mdl_lasso, as.matrix(X_test))))^2 ) / length(y)

## summary
MSE <- data.frame(OLS = MSE_lm, Ridge = MSE_ridge, Lasso = MSE_lasso)
rownames(MSE) <- "MSE"

MSE

```


#### \* 交差検証によるハイパーパラメータのチューニング
Ridge回帰及びLasso回帰のモデリングではハイパーパラメータ$\lambda$の最適な値を選定するためにtraining dataの中で交差検証が行われている(Nested Cross-Validation)．ただし，ハイパーパラメータのチューニングにおける交差検証は10-foldと呼ばれる手法を用いており，予測誤差の評価で用いた手法(holdout)とは異なる．


